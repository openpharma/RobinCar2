---
title: "Design Doc for Survival Analysis with RobinCar2"
author: Daniel Sabanes Bove
bibliography: references.bib
number-sections: true
---

This design doc shall serve to align on the design of the survival analysis implementation in RobinCar2.

# User interface

The proposal for the user interface is the function call of the form:

```{r}
#| eval: false

robin_surv(
    Surv(time, event) ~ treatment * s1 + covariates,
    data = df,
    treatment = pb(s1),
    contrast = "hazardratio",
    pval = "logrank"
)
```

## Name and arguments

- The name `robin_surv()` indicates general survival analysis functionality. In particular, we do not need to assume a Cox proportional hazards model in order to interpret the logrank test results. 
- We use the standard `Surv(time, event)` syntax from the `survival` package, with which users are familiar already.
    - If the user would not like to adjust for covariates, they can omit the `covariates` in the formula.
    - If the user would not like to have a stratified estimate, they can omit the `s1` in the formula and in the `treatment` argument.
- We keep in line with the other RobinCar2 functions for specifying the randomization scheme via the `treatment` argument.
- For the `contrast` argument, we will start only with the hazard ratio option, as laid out in the paper by @YeShaoYi2023. 
    - In the future, this could include other contrasts, such as differences of restricted mean survival times or differences of survival probabilities at a given time point.
- The `pval` argument allows the user to specify the type of p-value they want to compute. We will start with the logrank test, but in the future we could add other options, such as p-values based on the Cox score test, which is available in the RobinCar package already.

## Output

This could be the output from the `robin_surv()` function:

````
#> Model        :  Surv(time, event) ~ treatment * s1 + covariates
#> Randomization:  treatment ~ pb(s1)  ( Permuted-Block )
#> 
#> Contrast     :  Hazard ratio
#> Test         :  Logrank
#>                Estimate Std.Err Z Value  Pr(>|z|)    
#> trt1 v.s. pbo   0.56365 0.10074  5.5952 2.203e-08 ***
#> trt2 v.s. pbo   0.77093 0.10133  7.6082   0.05235 . 
#> trt2 v.s. trt1  0.20728 0.10683  1.9402 2.779e-14 *** 
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
````

Please note that for now there will be no "Marginal Mean" section included in the output. 
However, this could be added in the future, see @sec-questions below.

# Internal implementation

The `robincar_surv()` will follow a similar structure as e.g. `robincar_glm()`. However, the methods are quite different, e.g. here we do not have the prediction of counterfactuals. Therefore the pattern still looks quite different.

Steps include:

1. Checks
1. Calculation of risk set and failure times
1. For each pair of treatment arms, we will compute:
   a. unadjusted hazard ratio estimate
   a. covariate adjusted hazard ratio estimate
   a. standard error
   a. p-value for the logrank test
1. Return the list with results, with class `surv_effect`

Note that the class of the value is not `treatment_effect` here, because we don't have marginal means, or Jacobian matrix here, in contrast to GLMs. Since there is some overlap, which could e.g. be used for the `print()` method or others, it could be helpful to have a common class `treatment_effect` and then have subclasses `glm_effect` and `surv_effect` which contain additional information.

# Questions {#sec-questions}

1. For the future, which kind of marginal means could we display? E.g. restricted mean survival times which are also covariate and stratification adjusted would be great. 
    - This would then eventually parallel the results we obtain for GLMs
    - Does the methodology exist already? E.g. from references in @MagirrWangDengMorrisBaillie2024 ?
1. If we have more than two treatment arms in the data set, we would currently just compute the pairwise contrasts. 
    - Is there a more efficient way to compute the pairwise hazard ratios and tests maybe?
    - Is this sufficient? Or do we need to / could we compute an overall (logrank) test as well?    
1. How should ties be handled?
    - The "Breslow approximation" seems simplest, see e.g. in the `survival` package's manual:

        "The Breslow approximation in essence ignores the ties. If two subjects A and B died on day 100, say, the partial likelihood will have separate terms for the two deaths. Subject A will be at risk for the death that happened to B, and B will be at risk for the death that happened to A. In life this is not technically possible of course: whoever died first will not be at risk for the second death."
1. If a stratum only contains a single patient, do we count this patient in `n`?
    - Question arises because this stratum then does not contribute to the statistics (summand is 0).
    - Seems we still need to count it, at least like this we match the `survival` package's result exactly.
1. In the covariate adjusted hazard ratio estimate score function, should the regression coefficients really be fixed given the original estimate? I expected them to depend on the parameter $\vartheta$ as well, but both in the paper as well as in the supplementary material they are fixed at $\widehat{\beta}_{\text{L}}$ and $\widehat{\gamma}_{\text{SL}}$ respectively.

# Prototype

Idea is to code the prototype (almost) from scratch to have an independent implementation to compare with RobinCar, to really improve reliability. So I might still look at the overall flow of RobinCar and the helper functions, but I will not copy the code directly.

## Example data

As example data, we are going to use the NCCTG Lung Cancer Data set, which is available in the `survival` package.
We are going to use the male patients as treatment arm $j = 0$ and the female patients as treatment arm $j = 1$.

```{r}
library(dplyr)
library(checkmate)
dat <- survival::lung |>
    mutate(
        status = factor(ifelse(status == 1, "Alive", "Dead")),
        sex = factor(ifelse(sex == 1, "Male", "Female")),
        status_numeric = as.numeric(status == "Dead"),
        strata = factor(ph.ecog)
    )
head(dat)
```

## Unadjusted log-rank test

Let's first try to implement the unadjusted log-rank test, following the notation in Section 2 in @YeShaoYi2023.
We will see later when prototyping the stratified version why we need the `n` argument here.

```{r}
nonadj_log_rank <- function(df, treatment, time, status, n = nrow(df)) {
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)
    assert_count(n)
    assert_true(n >= nrow(df))

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead")
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Calculate the log rank statistic U_L and the variance sigma_L2 iteratively.
    U_L <- sigma_L2 <- 0
    for (i in seq_len(nrow(df_events))) {
        # This event time.
        t_i <- df_events$time[i]

        # This treatment arm indicator.
        I_i <- df_events$treatment[i]

        # Proportions of patients at risk at time t_i, per arm.
        Y_bar_1_ti <- sum(df$treatment & df$time >= t_i) / n
        Y_bar_0_ti <- sum(!df$treatment & df$time >= t_i) / n

        # Overall proportion of patients at risk at time t_i.
        Y_bar_ti <- Y_bar_0_ti + Y_bar_1_ti

        # Increment U_L.
        U_L <- U_L + (I_i - Y_bar_1_ti / Y_bar_ti)

        # Increment sigma_L2.
        sigma_L2 <- sigma_L2 + Y_bar_1_ti * Y_bar_0_ti / Y_bar_ti^2
    }

    U_L <- U_L / n
    sigma_L2 <- sigma_L2 / n
    tau_L <- sqrt(n) * U_L / sqrt(sigma_L2)
    pval <- 2 * pnorm(-abs(tau_L))
    list(
        U_L = U_L,
        sigma_L2 = sigma_L2,
        tau_L = tau_L,
        pval = pval
    )
}

our_res <- nonadj_log_rank(dat, "sex", "time", "status")
our_res
```

Let's compare this with the standard `survival` implementation:

```{r}
library(survival)

pkg_res <- survdiff(Surv(time, status_numeric) ~ sex, data = dat)
pkg_res

our_res$tau_L^2 - pkg_res$chisq
```

So this is pretty close.

Equivalently we can also compute the score test of the Cox model:

```{r}
check <- coxph(Surv(time, status_numeric) ~ sex, data = dat, ties = "breslow")
cox_res <- summary(check)$sctest["test"]
our_res$tau_L^2 - cox_res
```

So this is basically identical.

## Unadjusted hazard ratio estimate

Similarly as above, we can write down the score function from the partial likelihood for the Cox model with log hazard ratio $\theta$:

```{r}
nonadj_log_rank_score <- function(theta, df, treatment, time, status, n = nrow(df)) {
    assert_numeric(theta, min.len = 1L, finite = TRUE)
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead")
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Calculate the log rank statistic U_L and the variance sigma_L2 iteratively.
    U_L <- sigma_L2 <- 0
    for (i in seq_len(nrow(df_events))) {
        # This event time.
        t_i <- df_events$time[i]

        # This treatment arm indicator.
        I_i <- df_events$treatment[i]

        # Proportions of patients at risk at time t_i, per arm.
        Y_bar_1_ti <- mean(df$treatment & df$time >= t_i)
        Y_bar_0_ti <- mean(!df$treatment & df$time >= t_i)

        # Adjusted proportion of patients at risk in the treatment arm:
        Y_bar_1_ti_coxph <- Y_bar_1_ti * exp(theta)

        # Increment U_L.
        U_L <- U_L + (I_i - Y_bar_1_ti_coxph / (Y_bar_1_ti_coxph + Y_bar_0_ti))

        # Increment sigma_L2.
        sigma_L2 <- sigma_L2 + Y_bar_1_ti_coxph * Y_bar_0_ti / (Y_bar_1_ti_coxph + Y_bar_0_ti)^2
    }

    U_L <- U_L / n
    sigma_L2 <- sigma_L2 / n
    se_theta_L <- sqrt(1 / (n * sigma_L2))
    structure(U_L, sigma_L2 = sigma_L2, se_theta_L = se_theta_L)
}
```

We can see here that the only difference to the log rank test is the multiplication of $\bar{Y}_{1}(t)$ with the hazard ratio $\exp(\theta)$ in the score function.

### Standard error computation

While it is not explicitly written in @YeShaoYi2023, we can see from p. 698 that a variance estimate of the log hazard ratio estimate $\hat{\theta}_{\text{L}}$ is given by the inverse scaled negative derivative $1 / (n \cdot g(\vartheta))$ of the score function $\widehat{U}_{L}(\vartheta)$, i.e. $g(\vartheta) = \partial \widehat{U}_{L}(\vartheta)/ \partial\vartheta$, evaluated at the solution $\vartheta = \hat{\theta}_{\text{L}}$.

Here $g(\vartheta)$ is given by

$$
g(\vartheta) = \frac{1}{n} \sum_{i=1}^{n} \int_0^{\tau} 
\frac{e^{\vartheta} \overline{Y}_1(t) \overline{Y}_0(t)}
{\left( e^{\vartheta} \overline{Y}_1(t) + \overline{Y}_0(t) \right)^2}
dN_i(t)
$$

This matches the form we had used above for $\sigma_L^2$, merely replacing $\overline{Y}_1(t)$ by $e^{\vartheta} \overline{Y}_1(t)$.

Given $\sigma^2_L(\hat{\theta}_{\text{L}}) = 1 / (n \cdot g(\hat{\theta}_{\text{L}}))$, the standard error is then just the square root of that, and we can use it to construct confidence intervals based on the normal approximation. We have already included this calculation in the above function.

### Comparison with `survival` results

Let's see if we can match the `survival` results accordingly, by solving the score equation for $\theta$ to obtain the hazard ratio estimate:

```{r}
score_solution <- uniroot(
    nonadj_log_rank_score,
    interval = c(-10, 10),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status"
)
our_theta_est <- score_solution$root
our_theta_est

our_theta_est_se <- attr(score_solution$f.root, "se_theta_L")
our_theta_est_se
```

From the `survival` package we get the log hazard ratio estimate:

```{r}
pkg_theta_est <- as.numeric(coef(check))
pkg_theta_est
```

So this is indeed very close:

```{r}
our_theta_est - pkg_theta_est
```

And we get the standard error:

```{r}
pkg_theta_est_se <- sqrt(vcov(check))
```

We can compare that too:

```{r}
pkg_theta_est_se - our_theta_est_se
```

So that also matches well.

## Covariate adjusted log-rank test

Now we will follow Section 3 in @YeShaoYi2023 to implement the covariate adjusted log-rank test.

### Derived outcome values

First we need to calculate the derived outcome values, the $\hat{O}_{ij}$, for each patient $i$ in each treatment arm $j$, as defined in Equation (3) in the paper. If we denote all unique observed event times by $t_{(1)} < \dotsb < t_{(m)}$, then the $\hat{O}_{ij}$ value is practically calculated as:

$$
\hat{O}_{ij} = \sum_{k=1}^{m} 
\frac{\overline{Y}_{1-j}(t_{(k)})}{\overline{Y}(t_{(k)})} 
\left\{ dN_{ij}(t_{(k)}) - Y_{ij}(t_{(k)}) \frac{d\overline{N}(t_{(k)})}{\overline{Y}(t_{(k)})} \right\}
$$

Here we have the following quantities:

- $\overline{Y}_{1-j}(t_{(k)})$: the proportion of patients at risk in the opposite treatment arm at time $t_{(k)}$.
- $\overline{Y}(t_{(k)})$: the overall proportion of patients at risk at time $t_{(k)}$.
- $dN_{ij}(t_{(k)})$: whether this patient had an event at time $t_{(k)}$, then 1, otherwise 0.
- $Y_{ij}(t_{(k)})$: whether this patient was at risk at time $t_{(k)}$, then 1, otherwise 0.
- $d\overline{N}(t_{(k)})$: the proportion of patients having an event at time $t_{(k)}$.
- $\overline{Y}(t_{(k)})$: the overall proportion of patients at risk at time $t_{(k)}$.

One consideration here is what happens if there are ties between some of the event times. In that case, $N(t)$ would only jump at the unique event times, and the jump height would depend on the number of patients having an event at that time. Here it is important that the sum is across only the unique event times, and not across all event times.

We also note that all patients who have an event at the same time have the same $\hat{O}_{ij}$ value, and all patients who are censored at the same time have the same $\hat{O}_{ij}$ value. To perform this correctly, the sum 

This function takes the data frame with the meta data, now including the covariates specification (keeping it simple for now), sorts it by time and then adds the $\hat{O}_{ij}$ values to it. It returns the data in original order.

```{r}
derived_outcome_vals <- function(df, treatment, time, status, covariates, n = nrow(df)) {
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_character(covariates)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)
    assert_count(n)
    assert_true(n >= nrow(df))

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        index = seq_len(nrow(df)),
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead"),
        df[covariates]
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Unique observed event times.
    unique_times <- df_events$time |>
        split(df_events$time) |>
        sapply(length)
    df_events_unique <- data.frame(
        time = as.numeric(names(unique_times)),
        n_pts = as.numeric(unique_times)
    )

    # Add derived outcome column.
    df$O_hat <- NA_real_

    # Calculate quantities which are the same across patients first.
    # These are in parallel to df_events_unique.

    # Proportions of patients at risk, per unique event time and treatment arm.
    # Corresponds to \bar{Y}_1(t) and \bar{Y}_0(t).
    at_risk_matrix <- outer(df$time, df_events_unique$time, FUN = ">=")
    Y_bar_1 <- colSums(df$treatment & at_risk_matrix) / n
    Y_bar_0 <- colSums(!df$treatment & at_risk_matrix) / n
    Y_bar <- Y_bar_0 + Y_bar_1

    # Proportion of patients having an event at this time.
    # Corresponds to d\bar{N}(t). Here we need to be careful about tied event times,
    # therefore we see how many patients have an event at each unique time and divide that by n.
    dN_bar <- df_events_unique$n_pts / n

    # Loop over all patients.
    for (i in df$index) {
        # Treatment arm?
        I_i <- df$treatment[i]

        # Event in this patient?
        delta_i <- df$status[i] == 1L

        # Time for this patient.
        t_i <- df$time[i]

        # Does this patient have an event at this unique event time? Corresponds to dN_ij(t).
        dN_ij <- delta_i * (df_events_unique$time == t_i)

        # Is this patient at risk at this unique event time? Corresponds to Y_ij(t).
        Y_ij <- as.numeric(t_i >= df_events_unique$time)

        # Calculate the weights, Y_bar in opposite treatment arm divided by Y_bar overall.
        weights <- (I_i * Y_bar_0 + (1 - I_i) * Y_bar_1) / Y_bar

        # Compute martingale residuals.
        martingale_residuals <- dN_ij - Y_ij * dN_bar / Y_bar

        # Sum across all unique event times.
        df$O_hat[i] <- sum(weights * martingale_residuals)
    }

    df[order(df$index), ]
}
```

Let's try this out:

```{r}
dat_derived <- derived_outcome_vals(dat, "sex", "time", "status", covariates = c("age", "ph.karno", "meal.cal"))
head(dat_derived)
```

We can also see that the $\hat{O}_{ij}$ values here are the same for the patients with tied event times:

```{r}
unique_times_status <- table(dat_derived$time, dat_derived$status)
which(unique_times_status > 1, arr.ind = TRUE)
subset(dat_derived, time == 225 & status == 0)
subset(dat_derived, time == 11 & status == 1)
```

### Regression of derived outcome values on covariates

We can see from equation (5) in @YeShaoYi2023 that we can use a linear regression model, separately per treatment arm, and without intercept and with centered design matrix, to estimate the coefficients $\hat{\beta}_{j}$ for each treatment arm $j$.

These estimated coefficients are then crucial for the adjustment of the log rank test statistic in equation (4).

Let's first write a function that returns the original, i.e. non-centered, design matrices and the response for the two treatment arms: 

```{r}
get_lm_input <- function(df, model) {
    assert_data_frame(df)
    assert_formula(model)
    assert_subset(all.vars(model), names(df))

    # Add outcome, remove intercept:
    model <- update(model, O_hat ~ . - 1)
    df_by_trt <- split(df, f = df$treatment)
    lapply(
        df_by_trt,
        function(this_df) {
            mf <- model.frame(model, data = this_df)
            X <- model.matrix(model, data = mf)
            y <- model.response(mf)
            list(X = X, y = y)
        }
    )
}
```

Let's try this:

```{r}
lm_input <- get_lm_input(dat_derived, ~ age + ph.karno + meal.cal)
str(lm_input)
```

Note that here we might drop patients which have missing values in any of the covariates included in the model. Therefore it is important that we return the parallel response vector as well.

Now we can estimate the regression coefficients, using this input:

```{r}
get_beta_estimates <- function(lm_input) {
    assert_list(lm_input, types = "list")
    assert_names(names(lm_input), identical.to = c("0", "1"))

    # Fit the model separately for each treatment arm.
    beta_est <- list()

    for (group in names(lm_input)) {
        # Get the design matrix for this treatment arm.
        X <- lm_input[[group]]$X

        # Center it.
        X <- scale(X, center = TRUE, scale = FALSE)

        # Get the derived outcome values, the response.
        y <- lm_input[[group]]$y

        # Fit the model without intercept.
        lm_fit <- lm.fit(X, y)

        # Get the coefficients.
        beta_est[[group]] <- lm_fit$coefficients
    }

    beta_est
}
```

Let's try this:

```{r}
beta_est <- get_beta_estimates(lm_input)
beta_est
```

### Adjusted test statistic

Now, finally, we can implement the covariate adjusted log-rank test statistic based on equations (4) and (6) in @YeShaoYi2023.
We need to be careful in formula (4): It can be (as in the example here) that not all patients enter the linear model computations, because of missing values in some of their covariates. Therefore, when calculating the unadjusted result, we need to use the same patients as in the linear model.

```{r}
covadj_log_rank <- function(df, treatment, time, status, model) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    df_with_covs_ovals <- derived_outcome_vals(df, treatment, time, status, covariates = all.vars(model))
    lm_input <- get_lm_input(df_with_covs_ovals, model)
    beta_est <- get_beta_estimates(lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- union(names(lm_input[["0"]]$y), names(lm_input[["1"]]$y))
    df_included <- df[rownames(df) %in% included_pts, ]

    unadj_res <- nonadj_log_rank(df_included, treatment, time, status)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    X_all <- rbind(lm_input[["0"]]$X, lm_input[["1"]]$X)
    X_bar <- colMeans(X_all)

    # Center the design matrices with this overall average.
    X_0 <- scale(lm_input[["0"]]$X, center = X_bar, scale = FALSE)
    X_1 <- scale(lm_input[["1"]]$X, center = X_bar, scale = FALSE)

    # Compute adjustment term for U_L.
    n <- nrow(df_included)
    U_L_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CL.
    U_CL <- unadj_res$U_L - U_L_adj_term

    # Compute adjustment term for sigma_L2.
    cov_X <- cov(X_all)
    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_L2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% cov_X %*% beta_est_sum)

    # Compute adjusted sigma_CL2.
    sigma_CL2 <- unadj_res$sigma_L2 - sigma_L2_adj_term

    # Compute adjusted test statistic.
    tau_CL <- sqrt(n) * U_CL / sqrt(sigma_CL2)

    pval <- 2 * pnorm(-abs(tau_CL))

    # Return results.
    list(
        U_CL = U_CL,
        sigma_CL2 = sigma_CL2,
        tau_CL = tau_CL,
        pval = pval,
        unadj_res = unadj_res
    )
}

our_res <- covadj_log_rank(
    dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
our_res
```

### Comparison with `RobinCar`

```{r}
library(RobinCar)
robincar_lr_result <- robincar_logrank(
    df = df_subset,
    treat_col = "sex",
    response_col = "time",
    event_col = "status_numeric",
    car_strata_cols = NULL,
    covariate_cols = c("age", "ph.karno", "meal.cal"),
    car_scheme = "simple",
    adj_method = "CL",
    ref_arm = "Female",
    p_trt = mean(df_subset$sex == "Male")
)
str(robincar_lr_result)

robincar_U_CL <- robincar_lr_result$result$U
robincar_U_CL

robincar_tau_CL <- robincar_lr_result$result$statistic
robincar_tau_CL
```

So we can compare this with our results above:

```{r}
our_res$U_CL - robincar_U_CL
our_res$tau_CL - robincar_tau_CL
```

This is relatively close, but not yet the same.

## Covariate adjusted hazard ratio estimate

We follow p. 698 in @YeShaoYi2023 to implement the covariate adjusted score function for the Cox model with log hazard ratio $\theta$:

$$
\widehat{U}_{\text{CL}}(\vartheta) = 
\widehat{U}_{\text{L}}(\vartheta) - 
\frac{1}{n} \sum_{i=1}^{n} \left\{ I_i (X_i - \bar{X})^\top \widehat{\beta}_1(\widehat{\theta}_{L}) - (1 - I_i)(X_i - \bar{X})^\top \widehat{\beta}_0(\widehat{\theta}_{L}) \right\},
$$

where $\widehat{\theta}_{L}$ is the maximum partial likelihood estimate of the unadjusted log hazard ratio, which we e.g. above implemented by root-finding the function `nonadj_log_rank_score`.

Note that the correction term is then fixed and not a function of $\vartheta$. This seems to be an approximation to keep the computations simple. The expectation of the correction term is zero (asymptotically), and the asymptotic variance is also the same as the one we would get if we would use the true $\widehat{\beta}_{j}(\vartheta)$ instead of the fixed $\widehat{\beta}_{j}(\widehat{\theta}_{L})$.

Nevertheless, we are also going to implement the full version where the correction term is a function of $\vartheta$:

$$
\widehat{U}_{\text{CL}}(\vartheta) = 
\widehat{U}_{\text{L}}(\vartheta) - 
\frac{1}{n} \sum_{i=1}^{n} \left\{ I_i (X_i - \bar{X})^\top \widehat{\beta}_1(\vartheta) - (1 - I_i)(X_i - \bar{X})^\top \widehat{\beta}_0(\vartheta) \right\},
$$

So we first need to have a function for that:

### Derived outcome values depending on $\vartheta$

All these function names are just placeholders for now, they can definitely be improved...

Let's write this function which is very close to the above `derived_outcome_vals()` but takes as first argument a single $\vartheta$ value and returns the derived outcome values based on this log hazard ratio value.

```{r}
derived_outcome_vals_from_theta <- function(theta, df, treatment, time, status, covariates, n = nrow(df)) {
    assert_number(theta)
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_character(covariates)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        index = seq_len(nrow(df)),
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead"),
        df[covariates]
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Unique observed event times.
    unique_times <- df_events$time |>
        split(df_events$time) |>
        sapply(length)

    df_events_unique <- data.frame(
        time = as.numeric(names(unique_times)),
        n_pts = as.numeric(unique_times)
    )

    # Add derived outcome column.
    df$O_hat <- NA_real_

    # Calculate quantities which are the same across patients first.
    # These are in parallel to df_events_unique.

    # Hazard ratio.
    exp_theta <- exp(theta)

    # Proportions of patients at risk, per unique event time and treatment arm.
    # Corresponds to \exp(\vartheta) * \bar{Y}_1(t) and \bar{Y}_0(t).
    # So here theta enters.
    at_risk_matrix <- outer(df$time, df_events_unique$time, FUN = ">=")
    Y_bar_1 <- exp_theta * colMeans(df$treatment & at_risk_matrix)
    Y_bar_0 <- colMeans(!df$treatment & at_risk_matrix)
    Y_bar <- Y_bar_0 + Y_bar_1

    # Proportion of patients having an event at this time.
    # Corresponds to d\bar{N}(t). Here we need to be careful about tied event times,
    # therefore we see how many patients have an event at each unique time and divide that by n.
    dN_bar <- df_events_unique$n_pts / n

    # Loop over all patients.
    for (i in df$index) {
        # Treatment arm?
        I_i <- df$treatment[i]

        # Event in this patient?
        delta_i <- df$status[i] == 1L

        # Time for this patient.
        t_i <- df$time[i]

        # Does this patient have an event at this unique event time? Corresponds to dN_ij(t).
        dN_ij <- delta_i * (df_events_unique$time == t_i)

        # Is this patient at risk at this unique event time? Corresponds to Y_ij(t).
        # Here theta enters, too.
        Y_ij <- as.numeric(t_i >= df_events_unique$time) * ifelse(I_i, exp_theta, 1)

        # Calculate the weights, Y_bar in opposite treatment arm divided by Y_bar overall.
        weights <- (I_i * Y_bar_0 + (1 - I_i) * Y_bar_1) / Y_bar

        # Compute martingale residuals.
        martingale_residuals <- dN_ij - Y_ij * dN_bar / Y_bar

        # Sum across all unique event times.
        df$O_hat[i] <- sum(weights * martingale_residuals)
    }

    df[order(df$index), ]
}
```

Let's try this out:

```{r}
dat_derived_theta <- derived_outcome_vals_from_theta(
    theta = 1,
    dat,
    "sex",
    "time",
    "status",
    covariates = c("age", "ph.karno", "meal.cal")
)
head(dat_derived_theta)
```

### Regression coefficients depending on $\vartheta$

Here we don't need a new function, because we can just use the `get_beta_estimates()` function from above on the derived outcome values we just calculated. The estimates don't depend on $\vartheta$ in another way, but just through $\hat{O}_{ij}(\vartheta)$, which we already calculated.

### Covariate adjusted score function in $\vartheta$

So now we can base on that code the covariate adjusted score function. We start with one that just takes a scalar $\vartheta$, because `uniroot()` does actually not need a vectorized function. 

We also add an option here to decide whether we want to use the approximation with $\hat{\beta}_{j}(\hat{\theta}_{L})$ or the full version with $\hat{\beta}_{j}(\vartheta)$: If `theta_hat` is provided, we use the approximation, otherwise we use the full version.
Later this can be programmed more efficiently such that the correction term does not need to be recomputed for every call, but for now we just keep it simple here.

```{r}
covadj_log_rank_score <- function(theta, df, treatment, time, status, model, theta_hat = NULL) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    theta_use <- if (is.null(theta_hat)) theta else theta_hat
    df_with_covs_ovals <- derived_outcome_vals_from_theta(
        theta = theta_use,
        df,
        treatment,
        time,
        status,
        covariates = all.vars(model)
    )
    lm_input <- get_lm_input(df_with_covs_ovals, model)
    beta_est <- get_beta_estimates(lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- union(names(lm_input[["0"]]$y), names(lm_input[["1"]]$y))
    df_included <- df[rownames(df) %in% included_pts, ]

    unadj_score <- nonadj_log_rank_score(theta, df_included, treatment, time, status)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    X_all <- rbind(lm_input[["0"]]$X, lm_input[["1"]]$X)
    X_bar <- colMeans(X_all)

    # Center the design matrices with this overall average.
    X_0 <- scale(lm_input[["0"]]$X, center = X_bar, scale = FALSE)
    X_1 <- scale(lm_input[["1"]]$X, center = X_bar, scale = FALSE)

    # Compute adjustment term for U_L.
    n <- nrow(df_included)
    U_L_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CL.
    U_CL <- as.numeric(unadj_score) - U_L_adj_term

    # Compute adjustment term for sigma_L2.
    cov_X <- cov(X_all)
    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_L2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% cov_X %*% beta_est_sum)

    # Compute standard error for theta estimate.
    g_theta_CL <- attr(unadj_score, "sigma_L2")
    var_theta_CL <- (g_theta_CL - sigma_L2_adj_term) / (g_theta_CL^2) / n
    se_theta_CL <- suppressWarnings(sqrt(var_theta_CL))

    structure(U_CL, se_theta_CL = se_theta_CL)
}

covadj_log_rank_score(
    theta = 0,
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
covadj_log_rank_score(
    theta = -5,
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
covadj_log_rank_score(
    theta = 5,
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
```

### Standard error computation

Here we can follow the lines of p. 698 in @YeShaoYi2023 again. We can see that we need a similar quadratic form depending on the regression coefficient estimates and the covariance matrix of the design matrix, as we had for the computation of $\sigma_{\text{CL}}^2$ above.
We have included this already in the above function.

### Comparison with unadjusted results

Let's try to use this to compute the covariate adjusted log hazard ratio estimate $\hat{\theta}_{\text{CL}}$. 
First in the version with the full correction term:

```{r}
covadj_score_solution <- uniroot(
    covadj_log_rank_score,
    interval = c(-5, 5),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
our_covadj_theta_est <- covadj_score_solution$root
our_covadj_theta_est
```

Let's compare this with the unadjusted hazard ratio estimate for the same, reduced, data set:

```{r}
df_subset <- na.omit(subset(dat, select = c(time, status, status_numeric, sex, age, ph.karno, meal.cal, strata)))
unadj_score_solution <- uniroot(
    nonadj_log_rank_score,
    interval = c(-5, 5),
    df = df_subset,
    treatment = "sex",
    time = "time",
    status = "status"
)
our_unadj_theta_est <- unadj_score_solution$root
our_unadj_theta_est
```

We can then compare this with the approximate correction term:

```{r}
covadj_score_solution_approx <- uniroot(
    covadj_log_rank_score,
    interval = c(-5, 5),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal,
    theta_hat = our_unadj_theta_est
)
our_covadj_theta_est_approx <- covadj_score_solution_approx$root
our_covadj_theta_est_approx
our_covadj_theta_est_approx - our_covadj_theta_est
```

So we can see that the approximation works quite well.

For the standard error, we get:

```{r}
our_covadj_theta_est_se <- attr(covadj_score_solution$f.root, "se_theta_CL")
our_covadj_theta_est_se
```

We can compare that with the above calculated unadjusted standard error:

```{r}
our_theta_est_se <- attr(unadj_score_solution$f.root, "se_theta_L")
our_theta_est_se
```

So we can see that the covariate adjusted standard error is indeed smaller than the unadjusted one.

Also here we can compare with the results from using the approximate correction term:

```{r}
our_covadj_theta_est_se_approx <- attr(covadj_score_solution_approx$f.root, "se_theta_CL")
our_covadj_theta_est_se_approx
our_covadj_theta_est_se_approx - our_covadj_theta_est_se
```

### Comparison with `survival` results

Let's compare this with the `survival` package results using a Cox model:

```{r}
check_adjusted <- coxph(Surv(time, status_numeric) ~ sex + age + ph.karno + meal.cal, data = dat, ties = "breslow")

coxph_covadj_theta_est <- as.numeric(coef(check_adjusted)["sexMale"])
coxph_covadj_theta_est

coxph_covadj_theta_est_se <- sqrt(vcov(check_adjusted)["sexMale", "sexMale"])
coxph_covadj_theta_est_se
```

So these results are different, which is expected, because with this Cox model we are estimating a conditional hazard ratio.

### Comparison with `RobinCar` results

We can also compare this with the results from RobinCar:

```{r}
library(RobinCar)
cl <- robincar_covhr(
    df = df_subset,
    treat_col = "sex",
    response_col = "time",
    event_col = "status_numeric",
    car_strata_cols = NULL,
    covariate_cols = c("age", "ph.karno", "meal.cal"),
    car_scheme = "simple",
    adj_method = "CL",
    ref_arm = "Female",
    p_trt = mean(df_subset$sex == "Male")
)
str(cl)

robincar_theta_est <- cl$result$theta_CL
robincar_theta_est

robincar_theta_est_se <- cl$result$se_theta_CL
robincar_theta_est_se
```

So these results are very close but not numerically equal to what we obtained earlier:

```{r}
our_covadj_theta_est - robincar_theta_est
our_covadj_theta_est_se - robincar_theta_est_se

our_covadj_theta_est_approx - robincar_theta_est
our_covadj_theta_est_se_approx - robincar_theta_est_se
```

The standard error is closer, but the estimate is still a bit further off.

## Stratified log-rank test

Let's start looking into the stratified versions with the standard stratified log-rank test. Looking at formula (7) in @YeShaoYi2023, we can see that $\widehat{U}_{\text{SL}}$ is the sum of the non-stratified statistic $\widehat{U}_{\text{L}}$ calculated for each stratum $z$. Similar, the formula for $\widehat{\sigma}^{2}_{\text{SL}}$ is the sum of the strata specific statistics.

Important is that here the denominator $n$ stays constant and equal to the total number of patients across strata. Therefore we have adapted the `nonadj_log_rank()` function to take an additional argument `n` which we can now use here explicitly.

For passing the strata information, we use for now a single factor variable, where each level corresponds to a stratum.

```{r}
strat_log_rank <- function(df, treatment, time, status, strata) {
    assert_string(strata)
    assert_data_frame(df)
    assert_factor(df[[strata]])
    n <- nrow(df)

    df <- na.omit(df[, c(treatment, time, status, strata)])
    df[[strata]] <- droplevels(df[[strata]])
    strata_levels <- levels(df[[strata]])

    df_split <- split(df, f = df[[strata]])
    strata_results <- lapply(
        df_split,
        FUN = nonadj_log_rank,
        treatment = treatment,
        time = time,
        status = status,
        n = n
    )

    U_SL <- sum(sapply(strata_results, "[[", "U_L"))
    sigma_SL2 <- sum(sapply(strata_results, "[[", "sigma_L2"))
    tau_SL <- sqrt(n) * U_SL / sqrt(sigma_SL2)
    pval <- 2 * pnorm(-abs(tau_SL))
    list(
        U_SL = U_SL,
        sigma_SL2 = sigma_SL2,
        tau_SL = tau_SL,
        pval = pval
    )
}
```

Let's try this out - we just create a strata variable first:

```{r}
dat$strata <- factor(dat$ph.ecog)
our_strat_res <- strat_log_rank(dat, "sex", "time", "status", "strata")
our_strat_res
```

We note that in the case that one stratum has only a single patient, then this stratum does not contribute to the test statistics.
Here this is the case for stratum `ph.ecog = 3`.

Let's compare this with the standard packages' results:

```{r}
pkg_strat_res <- survdiff(Surv(time, status_numeric) ~ sex + strata(strata), data = dat)
pkg_strat_res

our_strat_res$tau_SL^2 - pkg_strat_res$chisq
```

So this is pretty close.

Equivalently we can also compute the score test of the Cox model:

```{r}
strat_check <- coxph(Surv(time, status_numeric) ~ sex + strata(strata), data = dat, ties = "breslow")
strat_cox_res <- summary(strat_check)$sctest["test"]
our_strat_res$tau_SL^2 - strat_cox_res
```

Again this is basically identical. So this works.

## Stratified hazard ratio estimate

Similarly as above, we can write down the score function from the partial likelihood for the stratified Cox model with log hazard ratio $\theta$. This is following Section~S2.3 in the supplementary material of @YeShaoYi2023, which shows that the score function is the sum of the strata specific score functions.

```{r}
strat_log_rank_score <- function(theta, df, treatment, time, status, strata) {
    assert_string(strata)
    assert_data_frame(df)
    assert_factor(df[[strata]])
    n <- nrow(df)

    df <- na.omit(df[, c(treatment, time, status, strata)])
    df[[strata]] <- droplevels(df[[strata]])
    strata_levels <- levels(df[[strata]])

    df_split <- split(df, f = df[[strata]])
    strata_results <- lapply(
        df_split,
        FUN = nonadj_log_rank_score,
        theta = theta,
        treatment = treatment,
        time = time,
        status = status,
        n = n
    )

    U_SL <- sum(sapply(strata_results, as.numeric))
    sigma_SL2 <- sum(sapply(strata_results, attr, "sigma_L2"))
    se_theta_SL <- sqrt(1 / (n * sigma_SL2))
    structure(U_SL, sigma_SL2 = sigma_SL2, se_theta_SL = se_theta_SL)
}
```

Let's try to use this to compute the stratified log hazard ratio estimate $\hat{\theta}_{\text{SL}}$:

```{r}
strat_score_solution <- uniroot(
    strat_log_rank_score,
    interval = c(-5, 5),
    df = dat,
    "sex",
    "time",
    "status",
    "strata"
)
strat_score_solution
our_strat_theta_est <- strat_score_solution$root
our_strat_theta_est_se <- attr(strat_score_solution$f.root, "se_theta_SL")
```

We can compare this with the `survival` package results:

```{r}
strat_cox_check <- coxph(Surv(time, status_numeric) ~ sex + strata(strata), data = dat, ties = "breslow")
coef(strat_cox_check) - our_strat_theta_est
sqrt(vcov(strat_cox_check)["sexMale", "sexMale"]) - our_strat_theta_est_se
```

So this matches very well.

## Covariate adjusted stratified log-rank test

### Derived outcome values

For the covariate adjusted version of the stratified log-rank test, we first calculate the derived outcome values. Similarly as above, we can reuse the `derived_outcome_vals()` function, but we have adapted it here too to take the `n` argument, which is the total number of patients across strata.

```{r}
strat_derived_outcome_vals <- function(df, treatment, time, status, strata, covariates) {
    assert_string(strata)
    assert_data_frame(df)
    assert_factor(df[[strata]])
    n <- nrow(df)

    df <- na.omit(df[, c(treatment, time, status, strata, covariates)])
    df[[strata]] <- droplevels(df[[strata]])
    strata_levels <- levels(df[[strata]])

    df_split <- split(df, f = df[[strata]])

    lapply(
        df_split,
        FUN = derived_outcome_vals,
        treatment = treatment,
        time = time,
        status = status,
        covariates = covariates,
        n = n
    )
}
```

So we just return the stratum specific derived outcome values, but we do not combine them yet.

Let's try this out:

```{r}
strat_dat_derived <- strat_derived_outcome_vals(
    dat,
    "sex",
    "time",
    "status",
    "strata",
    covariates = c("age", "ph.karno", "meal.cal")
)
str(strat_dat_derived, 2)
```

### Regression of derived outcome values on covariates

Unfortunately we cannot just sum up the strata specific correction terms, because we need to first derive global regression coefficients.

Again we start with the function to get the design matrices and response vectors, where we just process each stratum separately:

```{r}
get_strat_lm_input <- function(df_split, model) {
    assert_list(df_split, types = "data.frame")
    lapply(df_split, get_lm_input, model = model)
}
```

Let's try this:

```{r}
strat_lm_input <- get_strat_lm_input(strat_dat_derived, ~ age + ph.karno + meal.cal)
str(strat_lm_input, 2)
```

So for each stratum we have the treatment arm specific design matrices and response vectors.

Now we can estimate the global (across all strata) regression coefficients for each treatment arm. Here we cannot delegate to the function `get_beta_estimates()` from above, but instead need to perform a more manual coefficient estimate calculation:

```{r}
get_strat_beta_estimates <- function(strat_lm_input) {
    assert_list(strat_lm_input, types = "list")
    assert_list(strat_lm_input[[1]], types = "list")
    assert_names(names(strat_lm_input[[1]]), identical.to = c("0", "1"))

    # Get coefficient estimates separately for each treatment arm.
    beta_est <- list()

    for (group in c("0", "1")) {
        XtXs <- list()
        Xtys <- list()

        for (stratum in names(strat_lm_input)) {
            if (group %in% names(strat_lm_input[[stratum]])) {
                # Get the design matrix for this treatment arm.
                X <- strat_lm_input[[stratum]][[group]]$X

                # Center it.
                X <- scale(X, center = TRUE, scale = FALSE)

                # Get the derived outcome values, the response.
                y <- strat_lm_input[[stratum]][[group]]$y

                # Save the cross products.
                XtXs[[stratum]] <- crossprod(X)
                Xtys[[stratum]] <- crossprod(X, y)
            }
        }

        # Sum across strata.
        XtX <- Reduce("+", XtXs)
        Xty <- Reduce("+", Xtys)

        # Get the coefficients.
        beta_est[[group]] <- solve(XtX, Xty)
    }

    beta_est
}
```

Alternatively, we could also stack (row-bind) the design matrices and response vectors across strata, for each treatment group, and then use `lm.fit()` to obtain the coefficients. We can consider that for packaging.

Let's try this:

```{r}
strat_beta_est <- get_strat_beta_estimates(strat_lm_input)
strat_beta_est
```

### Adjusted test statistic

Now can implement the covariate adjusted stratified log-rank test statistic based on the first equation on p. 700 in @YeShaoYi2023.
We see that the formula is essentially the same as the one for the non-stratified version, and we just use stratum specific centering for the design matrices.

```{r}
strat_covadj_log_rank <- function(df, treatment, time, status, strata, model) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    df_split_with_covs_ovals <- strat_derived_outcome_vals(
        df,
        treatment,
        time,
        status,
        strata = strata,
        covariates = all.vars(model)
    )
    strat_lm_input <- get_strat_lm_input(df_split_with_covs_ovals, model)
    beta_est <- get_strat_beta_estimates(strat_lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- unlist(lapply(strat_lm_input, \(l) union(names(l[["0"]]$y), names(l[["1"]]$y))))
    df_included <- df[rownames(df) %in% included_pts, ]

    unadj_strat_res <- strat_log_rank(df_included, treatment, time, status, strata)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    strat_X_all <- lapply(strat_lm_input, \(l) rbind(l[["0"]]$X, l[["1"]]$X))
    strat_X_bar <- lapply(strat_X_all, colMeans)

    # Center the design matrices with this overall average.
    has_X_0 <- names(which(sapply(strat_lm_input, \(l) "0" %in% names(l))))
    has_X_1 <- names(which(sapply(strat_lm_input, \(l) "1" %in% names(l))))

    X_0 <- lapply(has_X_0, \(n) scale(strat_lm_input[[n]][["0"]]$X, center = strat_X_bar[[n]], scale = FALSE))
    X_1 <- lapply(has_X_1, \(n) scale(strat_lm_input[[n]][["1"]]$X, center = strat_X_bar[[n]], scale = FALSE))

    X_0 <- do.call(rbind, X_0)
    X_1 <- do.call(rbind, X_1)

    # Compute adjustment term for U_SL.
    n <- nrow(df_included)
    U_SL_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CSL.
    U_CSL <- unadj_strat_res$U_SL - U_SL_adj_term

    # Compute adjustment term for sigma_SL2.
    strat_n <- sapply(strat_X_all, nrow)
    strat_use <- names(which(strat_n > 1))
    strat_n <- strat_n[strat_use]
    overall_n <- sum(strat_n)
    strat_cov_X <- lapply(strat_X_all[strat_use], cov)
    weighted_cov_X <- Map(\(x, n) x * n / overall_n, strat_cov_X, strat_n)
    weighted_sum_cov_X <- Reduce("+", weighted_cov_X)

    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_SL2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% weighted_sum_cov_X %*% beta_est_sum)

    # Compute adjusted sigma_CSL2.
    sigma_CSL2 <- unadj_strat_res$sigma_SL2 - sigma_SL2_adj_term

    # Compute adjusted test statistic.
    tau_CSL <- sqrt(n) * U_CSL / sqrt(sigma_CSL2)

    pval <- 2 * pnorm(-abs(tau_CSL))

    # Return results.
    list(
        U_CSL = U_CSL,
        sigma_CSL2 = sigma_CSL2,
        tau_CSL = tau_CSL,
        pval = pval,
        unadj_strat_res = unadj_strat_res
    )
}

our_strat_covadj_res <- strat_covadj_log_rank(
    dat,
    treatment = "sex",
    time = "time",
    status = "status",
    strata = "strata",
    model = ~ age + ph.karno + meal.cal
)
our_strat_covadj_res
```

## Covariate adjusted stratified hazard ratio estimate

Here we need to look in the supplementary material of @YeShaoYi2023 in Section~S2.3, where they show the covariate adjusted score function for the stratified Cox model:

$$
\widehat{U}_{\text{CSL}}(\vartheta) = 
\widehat{U}_{\text{SL}}(\vartheta) - 
\frac{1}{n} \sum_{z} \sum_{i: Z_{i} = z} \left\{ I_i (X_i - \bar{X})^\top \widehat{\gamma}_1(\widehat{\theta}_{\text{SL}}) - (1 - I_i)(X_i - \bar{X})^\top \widehat{\gamma}_0(\widehat{\theta}_{\text{SL}}) \right\},
$$

where $\widehat{\theta}_{\text{SL}}$ is the maximum partial likelihood estimate of the unadjusted stratified log hazard ratio, and $\widehat{U}_{\text{SL}}(\vartheta)$ is the corresponding unadjusted stratified score function, which we implemented in `strat_log_rank_score`.

Similar as above we can also implement the full version where the correction term is a function of $\vartheta$:

$$
\widehat{U}_{\text{CSL}}(\vartheta) =
\widehat{U}_{\text{SL}}(\vartheta) -
\frac{1}{n} \sum_{z} \sum_{i: Z_{i} = z} \left\{ I_i (X_i - \bar{X})^\top \widehat{\gamma}_1(\vartheta) - (1 - I_i)(X_i - \bar{X})^\top \widehat{\gamma}_0(\vartheta) \right\},
$$

For this we again need a function that calculates the derived outcome values depending on $\vartheta$.

### Derived outcome values depending on $\vartheta$

Also here we can reuse the `derived_outcome_vals_from_theta` function from above, and just apply it to each stratum:

```{r}
strat_derived_outcome_vals_from_theta <- function(theta, df, treatment, time, status, strata, covariates) {
    assert_string(strata)
    assert_data_frame(df)
    assert_factor(df[[strata]])
    n <- nrow(df)

    df <- na.omit(df[, c(treatment, time, status, strata, covariates)])
    df[[strata]] <- droplevels(df[[strata]])
    strata_levels <- levels(df[[strata]])

    df_split <- split(df, f = df[[strata]])

    lapply(
        df_split,
        FUN = derived_outcome_vals_from_theta,
        theta = theta,
        treatment = treatment,
        time = time,
        status = status,
        covariates = covariates,
        n = n
    )
}
```

Again it is important here that we pass the `n` argument, which is the total number of patients across strata.

Let's try this out:

```{r}
strat_dat_derived_theta <- strat_derived_outcome_vals_from_theta(
    theta = 1,
    df = dat,
    "sex",
    "time",
    "status",
    "strata",
    covariates = c("age", "ph.karno", "meal.cal")
)
str(strat_dat_derived_theta, 2)
```

### Covariate adjusted stratified score function in $\vartheta$

Now we can finally implement the covariate adjusted stratified score function in $\vartheta$:

```{r}
strat_covadj_log_rank_score <- function(theta, df, treatment, time, status, strata, model, theta_hat = NULL) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    theta_use <- if (is.null(theta_hat)) theta else theta_hat
    df_split_with_covs_ovals <- strat_derived_outcome_vals_from_theta(
        theta = theta_use,
        df,
        treatment,
        time,
        status,
        strata,
        covariates = all.vars(model)
    )
    strat_lm_input <- get_strat_lm_input(df_split_with_covs_ovals, model)
    beta_est <- get_strat_beta_estimates(strat_lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- unlist(lapply(strat_lm_input, \(l) union(names(l[["0"]]$y), names(l[["1"]]$y))))
    df_included <- df[rownames(df) %in% included_pts, ]

    strat_unadj_score <- strat_log_rank_score(theta, df_included, treatment, time, status, strata)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    strat_X_all <- lapply(strat_lm_input, \(l) rbind(l[["0"]]$X, l[["1"]]$X))
    strat_X_bar <- lapply(strat_X_all, colMeans)

    # Center the design matrices with this overall average.
    has_X_0 <- names(which(sapply(strat_lm_input, \(l) "0" %in% names(l))))
    has_X_1 <- names(which(sapply(strat_lm_input, \(l) "1" %in% names(l))))

    X_0 <- lapply(has_X_0, \(n) scale(strat_lm_input[[n]][["0"]]$X, center = strat_X_bar[[n]], scale = FALSE))
    X_1 <- lapply(has_X_1, \(n) scale(strat_lm_input[[n]][["1"]]$X, center = strat_X_bar[[n]], scale = FALSE))

    X_0 <- do.call(rbind, X_0)
    X_1 <- do.call(rbind, X_1)

    # Compute adjustment term for U_SL.
    n <- nrow(df_included)
    U_SL_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CSL.
    U_CSL <- as.numeric(strat_unadj_score) - U_SL_adj_term

    # Compute adjustment term for sigma_SL2.
    strat_n <- sapply(strat_X_all, nrow)
    strat_use <- names(which(strat_n > 1))
    strat_n <- strat_n[strat_use]
    overall_n <- sum(strat_n)
    strat_cov_X <- lapply(strat_X_all[strat_use], cov)
    weighted_cov_X <- Map(\(x, n) x * n / overall_n, strat_cov_X, strat_n)
    weighted_sum_cov_X <- Reduce("+", weighted_cov_X)

    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_SL2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% weighted_sum_cov_X %*% beta_est_sum)

    # Compute standard error for theta estimate.
    g_theta_CSL <- attr(strat_unadj_score, "sigma_SL2")
    var_theta_CSL <- (g_theta_CSL - sigma_SL2_adj_term) / (g_theta_CSL^2) / n
    se_theta_CSL <- suppressWarnings(sqrt(var_theta_CSL))

    structure(U_CSL, se_theta_CSL = se_theta_CSL)
}

strat_covadj_log_rank_score(
    theta = 5,
    df = dat,
    "sex",
    "time",
    "status",
    "strata",
    model = ~ age + ph.karno + meal.cal
)
```

Let's do the root-finding to get the estimate. We will use the fixed correction term here so that we can best compare with the results from `RobinCar`.

```{r}
strat_covadj_score_solution <- uniroot(
    strat_covadj_log_rank_score,
    interval = c(-10, 10),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    strata = "strata",
    model = ~ age + ph.karno + meal.cal,
    theta_hat = our_strat_theta_est
)
strat_covadj_score_solution

our_strat_covadj_theta_est <- strat_covadj_score_solution$root
our_strat_covadj_theta_est

our_strat_covadj_theta_est_se <- attr(strat_covadj_score_solution$f.root, "se_theta_CSL")
our_strat_covadj_theta_est_se
```

### Comparison with `RobinCar` results

We can also compare this with the results from RobinCar:

```{r}
csl <- robincar_covhr(
    df = df_subset,
    treat_col = "sex",
    response_col = "time",
    event_col = "status_numeric",
    car_strata_cols = "strata", # Pass here the strata variables
    covariate_cols = c("age", "ph.karno", "meal.cal"),
    car_scheme = "biased-coin", # Must not be "simple" here
    adj_method = "CSL", # And here choose the strata adjustment
    ref_arm = "Female",
    p_trt = mean(df_subset$sex == "Male")
)
str(csl)

robincar_strat_theta_est <- csl$result$theta_CL
robincar_strat_theta_est

robincar_strat_theta_est_se <- csl$result$se_theta_CL
robincar_strat_theta_est_se
```

Here the function does not produce a standard error because of stratum 3 which only has a single patient.

So the results are still different:

```{r}
our_strat_covadj_theta_est - robincar_strat_theta_est
```

# References
