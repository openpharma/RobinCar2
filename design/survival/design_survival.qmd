---
title: "Design Doc for Survival Analysis with RobinCar2"
author: Daniel Sabanes Bove
bibliography: references.bib
number-sections: true
---

This design doc shall serve to align on the design of the survival analysis implementation in RobinCar2.

# User interface

The proposal for the user interface is the function call of the form:

```{r}
#| eval: false

robin_surv(
    Surv(time, event) ~ treatment * s1 + covariates,
    data = df,
    treatment = pb(s1),
    contrast = "hazardratio",
    pval = "logrank"
)
```

## Name and arguments

- The name `robin_surv()` indicates general survival analysis functionality. In particular, we do not need to assume a Cox proportional hazards model in order to interpret the logrank test results. 
- We use the standard `Surv(time, event)` syntax from the `survival` package, with which users are familiar already.
    - If the user would not like to adjust for covariates, they can omit the `covariates` in the formula.
    - If the user would not like to have a stratified estimate, they can omit the `s1` in the formula and in the `treatment` argument.
- We keep in line with the other RobinCar2 functions for specifying the randomization scheme via the `treatment` argument.
- For the `contrast` argument, we will start only with the hazard ratio option, as laid out in the paper by @YeShaoYi2023. 
    - In the future, this could include other contrasts, such as differences of restricted mean survival times or differences of survival probabilities at a given time point.
- The `pval` argument allows the user to specify the type of p-value they want to compute. We will start with the logrank test, but in the future we could add other options, such as p-values based on the Cox score test, which is available in the RobinCar package already.

## Output

This could be the output from the `robin_surv()` function:

````
#> Model        :  Surv(time, event) ~ treatment * s1 + covariates
#> Randomization:  treatment ~ pb(s1)  ( Permuted-Block )
#> 
#> Contrast     :  Hazard ratio
#> Test         :  Logrank
#>                Estimate Std.Err Z Value  Pr(>|z|)    
#> trt1 v.s. pbo   0.56365 0.10074  5.5952 2.203e-08 ***
#> trt2 v.s. pbo   0.77093 0.10133  7.6082   0.05235 . 
#> trt2 v.s. trt1  0.20728 0.10683  1.9402 2.779e-14 *** 
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
````

Please note that for now there will be no "Marginal Mean" section included in the output. 
However, this could be added in the future, see @sec-questions below.

# Internal implementation

The `robincar_surv()` will follow a similar structure as e.g. `robincar_glm()`. However, the methods are quite different, e.g. here we do not have the prediction of counterfactuals. Therefore the pattern still looks quite different.

Steps include:

1. Checks
1. Calculation of risk set and failure times
1. For each pair of treatment arms, we will compute:
   a. unadjusted hazard ratio estimate
   a. covariate adjusted hazard ratio estimate
   a. standard error
   a. p-value for the logrank test
1. Return the list with results, with class `surv_effect`

Note that the class of the value is not `treatment_effect` here, because we don't have marginal means, or Jacobian matrix here, in contrast to GLMs. Since there is some overlap, which could e.g. be used for the `print()` method or others, it could be helpful to have a common class `treatment_effect` and then have subclasses `glm_effect` and `surv_effect` which contain additional information.

# Questions {#sec-questions}

1. For the future, which kind of marginal means could we display? E.g. restricted mean survival times which are also covariate and stratification adjusted would be great. 
    - This would then eventually parallel the results we obtain for GLMs
    - Does the methodology exist already? E.g. from references in @MagirrWangDengMorrisBaillie2024 ?
1. If we have more than two treatment arms in the data set, we would currently just compute the pairwise contrasts. 
    - Is there a more efficient way to compute the pairwise hazard ratios and tests maybe?
    - Is this sufficient? Or do we need to / could we compute an overall (logrank) test as well?    
1. How should ties be handled?
    - The "Breslow approximation" seems simplest, see e.g. in the `survival` package's manual:

        "The Breslow approximation in essence ignores the ties. If two subjects A and B died on day 100, say, the partial likelihood will have separate terms for the two deaths. Subject A will be at risk for the death that happened to B, and B will be at risk for the death that happened to A. In life this is not technically possible of course: whoever died first will not be at risk for the second death."

# Prototype

Idea is to code the prototype (almost) from scratch to have an independent implementation to compare with RobinCar, to really improve reliability. So I might still look at the overall flow of RobinCar and the helper functions, but I will not copy the code directly.

## Example data

As example data, we are going to use the NCCTG Lung Cancer Data set, which is available in the `survival` package.
We are going to use the male patients as treatment arm $j = 0$ and the female patients as treatment arm $j = 1$.

```{r}
library(dplyr)
library(checkmate)
dat <- survival::lung |>
    mutate(
        status = factor(ifelse(status == 1, "Alive", "Dead")),
        sex = factor(ifelse(sex == 1, "Male", "Female")),
        status_numeric = as.numeric(status == "Dead")
    )
head(dat)
```

## Unadjusted log-rank test

Let's first try to implement the unadjusted log-rank test, following the notation in Section 2 in @YeShaoYi2023.

```{r}
nonadj_log_rank <- function(df, treatment, time, status) {
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead")
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Calculate the log rank statistic U_L and the variance sigma_L2 iteratively.
    U_L <- sigma_L2 <- 0
    for (i in seq_len(nrow(df_events))) {
        # This event time.
        t_i <- df_events$time[i]

        # This treatment arm indicator.
        I_i <- df_events$treatment[i]

        # Proportions of patients at risk at time t_i, per arm.
        Y_bar_1_ti <- mean(df$treatment & df$time >= t_i)
        Y_bar_0_ti <- mean(!df$treatment & df$time >= t_i)

        # Overall proportion of patients at risk at time t_i.
        Y_bar_ti <- Y_bar_0_ti + Y_bar_1_ti

        # Increment U_L.
        U_L <- U_L + (I_i - Y_bar_1_ti / Y_bar_ti)

        # Increment sigma_L2.
        sigma_L2 <- sigma_L2 + Y_bar_1_ti * Y_bar_0_ti / Y_bar_ti^2
    }
    n <- nrow(df)
    U_L <- U_L / n
    sigma_L2 <- sigma_L2 / n
    tau_L <- sqrt(n) * U_L / sqrt(sigma_L2)
    pval <- 2 * pnorm(-abs(tau_L))
    list(
        U_L = U_L,
        sigma_L2 = sigma_L2,
        tau_L = tau_L,
        pval = pval
    )
}

our_res <- nonadj_log_rank(dat, "sex", "time", "status")
our_res
```

Let's compare this with the standard `survival` implementation:

```{r}
library(survival)

pkg_res <- survdiff(Surv(time, status_numeric) ~ sex, data = dat)
pkg_res

our_res$tau_L^2 - pkg_res$chisq
```

So this is pretty close.

Equivalently we can also compute the score test of the Cox model:

```{r}
check <- coxph(Surv(time, status_numeric) ~ sex, data = dat, ties = "breslow")
cox_res <- summary(check)$sctest["test"]
our_res$tau_L^2 - cox_res
```

So this is basically identical.

## Unadjusted hazard ratio estimate

Similarly as above, we can write down the score function from the partial likelihood for the Cox model with log hazard ratio $\theta$:

```{r}
nonadj_log_rank_score <- function(theta, df, treatment, time, status) {
    assert_numeric(theta, min.len = 1L, finite = TRUE)
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead")
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Calculate the log rank statistic U_L and the variance sigma_L2 iteratively.
    U_L <- sigma_L2 <- 0
    for (i in seq_len(nrow(df_events))) {
        # This event time.
        t_i <- df_events$time[i]

        # This treatment arm indicator.
        I_i <- df_events$treatment[i]

        # Proportions of patients at risk at time t_i, per arm.
        Y_bar_1_ti <- mean(df$treatment & df$time >= t_i)
        Y_bar_0_ti <- mean(!df$treatment & df$time >= t_i)

        # Adjusted proportion of patients at risk in the treatment arm:
        Y_bar_1_ti_coxph <- Y_bar_1_ti * exp(theta)

        # Increment U_L.
        U_L <- U_L + (I_i - Y_bar_1_ti_coxph / (Y_bar_1_ti_coxph + Y_bar_0_ti))

        # Increment sigma_L2.
        sigma_L2 <- sigma_L2 + Y_bar_1_ti_coxph * Y_bar_0_ti / (Y_bar_1_ti_coxph + Y_bar_0_ti)^2
    }
    n <- nrow(df)
    U_L <- U_L / n
    sigma_L2 <- sigma_L2 / n
    se_theta_L <- sqrt(1 / (n * sigma_L2))
    structure(U_L, sigma_L2 = sigma_L2, se_theta_L = se_theta_L)
}
```

We can see here that the only difference to the log rank test is the multiplication of $\bar{Y}_{1}(t)$ with the hazard ratio $\exp(\theta)$ in the score function.

### Standard error computation

While it is not explicitly written in @YeShaoYi2023, we can see from p. 698 that a variance estimate of the log hazard ratio estimate $\hat{\theta}_{\text{L}}$ is given by the inverse scaled negative derivative $1 / (n \cdot g(\vartheta))$ of the score function $\widehat{U}_{L}(\vartheta)$, i.e. $g(\vartheta) = \partial \widehat{U}_{L}(\vartheta)/ \partial\vartheta$, evaluated at the solution $\vartheta = \hat{\theta}_{\text{L}}$.

Here $g(\vartheta)$ is given by

$$
g(\vartheta) = \frac{1}{n} \sum_{i=1}^{n} \int_0^{\tau} 
\frac{e^{\vartheta} \overline{Y}_1(t) \overline{Y}_0(t)}
{\left( e^{\vartheta} \overline{Y}_1(t) + \overline{Y}_0(t) \right)^2}
dN_i(t)
$$

This matches the form we had used above for $\sigma_L^2$, merely replacing $\overline{Y}_1(t)$ by $e^{\vartheta} \overline{Y}_1(t)$.

Given $\sigma^2_L(\hat{\theta}_{\text{L}}) = 1 / (n \cdot g(\hat{\theta}_{\text{L}}))$, the standard error is then just the square root of that, and we can use it to construct confidence intervals based on the normal approximation. We have already included this calculation in the above function.

### Comparison with `survival` results

Let's see if we can match the `survival` results accordingly, by solving the score equation for $\theta$ to obtain the hazard ratio estimate:

```{r}
score_solution <- uniroot(
    nonadj_log_rank_score,
    interval = c(-10, 10),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status"
)
our_theta_est <- score_solution$root
our_theta_est

our_theta_est_se <- attr(score_solution$f.root, "se_theta_L")
our_theta_est_se
```

From the `survival` package we get the log hazard ratio estimate:

```{r}
pkg_theta_est <- as.numeric(coef(check))
pkg_theta_est
```

So this is indeed very close:

```{r}
our_theta_est - pkg_theta_est
```

And we get the standard error:

```{r}
pkg_theta_est_se <- sqrt(vcov(check))
```

We can compare that too:

```{r}
pkg_theta_est_se - our_theta_est_se
```

So that also matches well.

## Covariate adjusted log-rank test

Now we will follow Section 3 in @YeShaoYi2023 to implement the covariate adjusted log-rank test.

### Derived outcome values

First we need to calculate the derived outcome values, the $\hat{O}_{ij}$, for each patient $i$ in each treatment arm $j$, as defined in Equation (3) in the paper. If we denote all observed event times by $t_{(1)}, \ldots, t_{(m)}$, then the $\hat{O}_{ij}$ value is practically calculated as:

$$
\hat{O}_{ij} = \sum_{k=1}^{m} 
\frac{\overline{Y}_{1-j}(t_{(k)})}{\overline{Y}(t_{(k)})} 
\left\{ dN_{ij}(t_{(k)}) - Y_{ij}(t_{(k)}) \frac{d\overline{N}(t_{(k)})}{\overline{Y}(t_{(k)})} \right\}
$$

Here we have the following quantities:

- $\overline{Y}_{1-j}(t_{(k)})$: the proportion of patients at risk in the opposite treatment arm at time $t_{(k)}$.
- $\overline{Y}(t_{(k)})$: the overall proportion of patients at risk at time $t_{(k)}$.
- $dN_{ij}(t_{(k)})$: whether this patient had an event at time $t_{(k)}$, then 1, otherwise 0.
- $Y_{ij}(t_{(k)})$: whether this patient was at risk at time $t_{(k)}$, then 1, otherwise 0.
- $d\overline{N}(t_{(k)})$: the proportion of patients having an event at time $t_{(k)}$.
- $\overline{Y}(t_{(k)})$: the overall proportion of patients at risk at time $t_{(k)}$.

One consideration here is what happens if there are ties between some of the event times. In that case, $N(t)$ would only jump at the unique event times, and the jump height would depend on the number of patients having an event at that time. However, since the other quantities are about proportions of patients at risk, and these would not change if we had ties, we can just ignore the ties for the purpose of calculating the $\hat{O}_{ij}$ values. We basically just sum the same summand more often then, according to how many patients had an event at that time.

This function takes the data frame with the meta data, now including the covariates specification (keeping it simple for now), sorts it by time and then adds the $\hat{O}_{ij}$ values to it.

```{r}
derived_outcome_vals <- function(df, treatment, time, status, covariates) {
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_character(covariates)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead"),
        df[covariates]
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Add derived outcome column.
    df$O_hat <- NA_real_

    # Calculate quantities which are the same across patients first.
    # These are in parallel to df_events.

    # Proportions of patients at risk, per event time and treatment arm.
    # Corresponds to \bar{Y}_1(t) and \bar{Y}_0(t).
    at_risk_matrix <- outer(df$time, df_events$time, FUN = ">=")
    Y_bar_1 <- colMeans(df$treatment & at_risk_matrix)
    Y_bar_0 <- colMeans(!df$treatment & at_risk_matrix)
    Y_bar <- Y_bar_0 + Y_bar_1

    # Proportion of patients having an event at this time.
    # Corresponds to d\bar{N}(t).
    dN_bar <- rep(1 / nrow(df), nrow(df_events))

    # Loop over all patients.
    for (i in seq_len(nrow(df))) {
        # Treatment arm?
        I_i <- df$treatment[i]

        # Event in this patient?
        delta_i <- df$status[i] == 1L

        # Time for this patient.
        t_i <- df$time[i]

        # Does this patient have an event at this time? Corresponds to dN_ij(t).
        dN_ij <- delta_i * (df_events$time == t_i)

        # Is this patient at risk at this time? Corresponds to Y_ij(t).
        Y_ij <- as.numeric(t_i >= df_events$time)

        # Calculate the weights, Y_bar in opposite treatment arm divided by Y_bar overall.
        weights <- (I_i * Y_bar_0 + (1 - I_i) * Y_bar_1) / Y_bar

        # Compute martingale residuals.
        martingale_residuals <- dN_ij - Y_ij * dN_bar / Y_bar

        # Sum across all event times.
        df$O_hat[i] <- sum(weights * martingale_residuals)
    }

    df
}
```

Let's try this out:

```{r}
dat_derived <- derived_outcome_vals(dat, "sex", "time", "status", covariates = c("age", "ph.karno", "meal.cal"))
head(dat_derived)
```

We can also see that the $\hat{O}_{ij}$ values here are the same for the patients with tied event times.

### Regression of derived outcome values on covariates

We can see from equation (5) in @YeShaoYi2023 that we can use a linear regression model, separately per treatment arm, and without intercept and with centered design matrix, to estimate the coefficients $\hat{\beta}_{j}$ for each treatment arm $j$.

These estimated coefficients are then crucial for the adjustment of the log rank test statistic in equation (4).

Let's first write a function that returns the original, i.e. non-centered, design matrices and the response for the two treatment arms: 

```{r}
get_lm_input <- function(df, model) {
    assert_data_frame(df)
    assert_formula(model)
    assert_subset(all.vars(model), names(df))

    # Add outcome, remove intercept:
    model <- update(model, O_hat ~ . - 1)
    df_by_trt <- split(df, f = df$treatment)
    lapply(
        df_by_trt,
        function(this_df) {
            mf <- model.frame(model, data = this_df)
            X <- model.matrix(model, data = mf)
            y <- model.response(mf)
            list(X = X, y = y)
        }
    )
}
```

Let's try this:

```{r}
lm_input <- get_lm_input(dat_derived, ~ age + ph.karno + meal.cal)
str(lm_input)
```

Note that here we might drop patients which have missing values in any of the covariates included in the model. Therefore it is important that we return the parallel response vector as well.

Now we can estimate the regression coefficients, using this input:

```{r}
get_beta_estimates <- function(lm_input) {
    assert_list(lm_input, types = "list")
    assert_names(names(lm_input), identical.to = c("0", "1"))

    # Fit the model separately for each treatment arm.
    beta_est <- list()

    for (group in names(lm_input)) {
        # Get the design matrix for this treatment arm.
        X <- lm_input[[group]]$X

        # Center it.
        X <- scale(X, center = TRUE, scale = FALSE)

        # Get the derived outcome values, the response.
        y <- lm_input[[group]]$y

        # Fit the model without intercept.
        lm_fit <- lm.fit(X, y)

        # Get the coefficients.
        beta_est[[group]] <- lm_fit$coefficients
    }

    beta_est
}
```

Let's try this:

```{r}
beta_est <- get_beta_estimates(lm_input)
beta_est
```

### Adjusted test statistic

Now, finally, we can implement the covariate adjusted log-rank test statistic based on equations (4) and (6) in @YeShaoYi2023.
We need to be careful in formula (4): It can be (as in the example here) that not all patients enter the linear model computations, because of missing values in some of their covariates. Therefore, when calculating the unadjusted result, we need to use the same patients as in the linear model.

```{r}
covadj_log_rank <- function(df, treatment, time, status, model) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    df_with_covs_ovals <- derived_outcome_vals(df, treatment, time, status, covariates = all.vars(model))
    lm_input <- get_lm_input(df_with_covs_ovals, model)
    beta_est <- get_beta_estimates(lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- union(names(lm_input[["0"]]$y), names(lm_input[["1"]]$y))
    df_included <- df[rownames(df) %in% included_pts, ]

    unadj_res <- nonadj_log_rank(df_included, treatment, time, status)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    X_all <- rbind(lm_input[["0"]]$X, lm_input[["1"]]$X)
    X_bar <- colMeans(X_all)

    # Center the design matrices with this overall average.
    X_0 <- scale(lm_input[["0"]]$X, center = X_bar, scale = FALSE)
    X_1 <- scale(lm_input[["1"]]$X, center = X_bar, scale = FALSE)

    # Compute adjustment term for U_L.
    n <- nrow(df_included)
    U_L_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CL.
    U_CL <- unadj_res$U_L - U_L_adj_term

    # Compute adjustment term for sigma_L2.
    cov_X <- cov(X_all)
    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_L2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% cov_X %*% beta_est_sum)

    # Compute adjusted sigma_CL2.
    sigma_CL2 <- unadj_res$sigma_L2 - sigma_L2_adj_term

    # Compute adjusted test statistic.
    tau_CL <- sqrt(n) * U_CL / sqrt(sigma_CL2)

    pval <- 2 * pnorm(-abs(tau_CL))

    # Return results.
    list(
        U_CL = U_CL,
        sigma_CL2 = sigma_CL2,
        tau_CL = tau_CL,
        pval = pval,
        unadj_res = unadj_res
    )
}

our_res <- covadj_log_rank(dat, treatment = "sex", time = "time", status = "status", model = ~ age + ph.karno + meal.cal)
our_res
```

## Covariate adjusted hazard ratio estimate

We follow p. 698 in @YeShaoYi2023 to implement the covariate adjusted score function for the Cox model with log hazard ratio $\theta$:
$$
\widehat{U}_{\text{CL}}(\vartheta) = 
\widehat{U}_{\text{L}}(\vartheta) - 
\frac{1}{n} \sum_{i=1}^{n} \left\{ I_i (X_i - \bar{X})^\top \widehat{\beta}_1(\vartheta) - (1 - I_i)(X_i - \bar{X})^\top \widehat{\beta}_0(\vartheta) \right\},
$$

where we fixed the typo that the regression coefficients depend on $\vartheta$ instead of $\widehat{\theta}_{\text{L}}$.
As mentioned in the paper, in turn they depend on derived outcomes calculated from $\vartheta$. So we first need to have a function for that:

### Derived outcome values depending on $\vartheta$

All these function names are just placeholders for now, they can definitely be improved...

Let's write this function which is very close to the above `derived_outcome_vals()` but takes as first argument a single $\vartheta$ value and returns the derived outcome values based on this log hazard ratio value.

```{r}
derived_outcome_vals_from_theta <- function(theta, df, treatment, time, status, covariates) {
    assert_number(theta)
    assert_string(treatment)
    assert_string(time)
    assert_string(status)
    assert_character(covariates)
    assert_data_frame(df)
    assert_factor(df[[treatment]], n.levels = 2L)
    assert_factor(df[[status]], levels = c("Alive", "Dead"))
    assert_numeric(df[[time]], lower = 0)

    # Standardize data set format, subset to relevant variables.
    df <- data.frame(
        treatment = as.numeric(df[[treatment]]) - 1L,
        time = df[[time]],
        status = as.numeric(df[[status]] == "Dead"),
        df[covariates]
    )

    # Sort by time.
    df <- df[order(df$time), ]

    # Patients with events.
    df_events <- df[df$status == 1L, ]

    # Add derived outcome column.
    df$O_hat <- NA_real_

    # Calculate quantities which are the same across patients first.
    # These are in parallel to df_events.

    # Hazard ratio.
    exp_theta <- exp(theta)

    # Proportions of patients at risk, per event time and treatment arm.
    # Corresponds to \exp(\vartheta) * \bar{Y}_1(t) and \bar{Y}_0(t).
    # So here theta enters.
    at_risk_matrix <- outer(df$time, df_events$time, FUN = ">=")
    Y_bar_1 <- exp_theta * colMeans(df$treatment & at_risk_matrix)
    Y_bar_0 <- colMeans(!df$treatment & at_risk_matrix)
    Y_bar <- Y_bar_0 + Y_bar_1

    # Proportion of patients having an event at this time.
    # Corresponds to d\bar{N}(t).
    dN_bar <- rep(1 / nrow(df), nrow(df_events))

    # Loop over all patients.
    for (i in seq_len(nrow(df))) {
        # Treatment arm?
        I_i <- df$treatment[i]

        # Event in this patient?
        delta_i <- df$status[i] == 1L

        # Time for this patient.
        t_i <- df$time[i]

        # Does this patient have an event at this time? Corresponds to dN_ij(t).
        dN_ij <- delta_i * (df_events$time == t_i)

        # Is this patient at risk at this time? Corresponds to Y_ij(t).
        # Here theta enters, too.
        Y_ij <- as.numeric(t_i >= df_events$time) * ifelse(I_i, exp_theta, 1)

        # Calculate the weights, Y_bar in opposite treatment arm divided by Y_bar overall.
        weights <- (I_i * Y_bar_0 + (1 - I_i) * Y_bar_1) / Y_bar

        # Compute martingale residuals.
        martingale_residuals <- dN_ij - Y_ij * dN_bar / Y_bar

        # Sum across all event times.
        df$O_hat[i] <- sum(weights * martingale_residuals)
    }

    df
}
```

Let's try this out:

```{r}
dat_derived_theta <- derived_outcome_vals_from_theta(theta = 1, dat, "sex", "time", "status", covariates = c("age", "ph.karno", "meal.cal"))
head(dat_derived_theta)
```

### Regression coefficients depending on $\vartheta$

Here we don't need a new function, because we can just use the `get_beta_estimates()` function from above on the derived outcome values we just calculated. The estimates don't depend on $\vartheta$ in another way, but just through $\hat{O}_{ij}(\vartheta)$, which we already calculated.

### Covariate adjusted score function in $\vartheta$

So now we can based on that code the covariate adjusted score function. We start with one that just takes a scalar $\vartheta$, because `uniroot()` does actually not need a vectorized function. 

```{r}
covadj_log_rank_score <- function(theta, df, treatment, time, status, model) {
    # Use row names for df such that we can see which patients are dropped etc.
    rownames(df) <- seq_len(nrow(df))

    # Calculate derived outcomes and regress them on covariates.
    df_with_covs_ovals <- derived_outcome_vals_from_theta(theta, df, treatment, time, status, covariates = all.vars(model))
    lm_input <- get_lm_input(df_with_covs_ovals, model)
    beta_est <- get_beta_estimates(lm_input)

    # Obtain unadjusted result for the patients included in the two linear models.
    included_pts <- union(names(lm_input[["0"]]$y), names(lm_input[["1"]]$y))
    df_included <- df[rownames(df) %in% included_pts, ]

    unadj_score <- nonadj_log_rank_score(theta, df_included, treatment, time, status)

    # We assume here that the observed proportion of treatment 1 in the data set corresponds to the preplanned
    # proportion of treatment 1 in the trial.
    pi <- mean(as.numeric(df[[treatment]]) - 1)

    # Overall column wise average of design matrices.
    X_all <- rbind(lm_input[["0"]]$X, lm_input[["1"]]$X)
    X_bar <- colMeans(X_all)

    # Center the design matrices with this overall average.
    X_0 <- scale(lm_input[["0"]]$X, center = X_bar, scale = FALSE)
    X_1 <- scale(lm_input[["1"]]$X, center = X_bar, scale = FALSE)

    # Compute adjustment term for U_L.
    n <- nrow(df_included)
    U_L_adj_term <- (sum(X_1 %*% beta_est[["1"]]) - sum(X_0 %*% beta_est[["0"]])) / n

    # Compute adjusted U_CL.
    U_CL <- as.numeric(unadj_score) - U_L_adj_term

    # Compute adjustment term for sigma_L2.
    cov_X <- cov(X_all)
    beta_est_sum <- beta_est[["0"]] + beta_est[["1"]]
    sigma_L2_adj_term <- pi * (1 - pi) * as.numeric(t(beta_est_sum) %*% cov_X %*% beta_est_sum)

    # Compute standard error for theta estimate.
    g_theta_CL <- attr(unadj_score, "sigma_L2")
    var_theta_CL <- (g_theta_CL - sigma_L2_adj_term) / (g_theta_CL^2) / n
    se_theta_CL <- suppressWarnings(sqrt(var_theta_CL))

    structure(U_CL, se_theta_CL = se_theta_CL)
}

covadj_log_rank_score(theta = 0, df = dat, treatment = "sex", time = "time", status = "status", model = ~ age + ph.karno + meal.cal)
covadj_log_rank_score(theta = -5, df = dat, treatment = "sex", time = "time", status = "status", model = ~ age + ph.karno + meal.cal)
covadj_log_rank_score(theta = 5, df = dat, treatment = "sex", time = "time", status = "status", model = ~ age + ph.karno + meal.cal)
```

### Standard error computation

Here we can follow the lines of p. 698 in @YeShaoYi2023 again. We can see that we need a similar quadratic form depending on the regression coefficient estimates and the covariance matrix of the design matrix, as we had for the computation of $\sigma_{\text{CL}}^2$ above.
We have included this already in the above function.

### Comparison with unadjusted results

Let's try to use this to compute the covariate adjusted log hazard ratio estimate $\hat{\theta}_{\text{CL}}$:

```{r}
covadj_score_solution <- uniroot(
    covadj_log_rank_score,
    interval = c(-5, 5),
    df = dat,
    treatment = "sex",
    time = "time",
    status = "status",
    model = ~ age + ph.karno + meal.cal
)
our_covadj_theta_est <- covadj_score_solution$root
our_covadj_theta_est
```

Let's compare this with the unadjusted hazard ratio estimate for the same, reduced, data set:

```{r}
df_subset <- na.omit(subset(dat, select = c(time, status, status_numeric, sex, age, ph.karno, meal.cal)))
unadj_score_solution <- uniroot(
    nonadj_log_rank_score,
    interval = c(-5, 5),
    df = df_subset,
    treatment = "sex",
    time = "time",
    status = "status"
)
our_unadj_theta_est <- unadj_score_solution$root
our_unadj_theta_est
```

For the standard error, we get:

```{r}
our_covadj_theta_est_se <- attr(covadj_score_solution$f.root, "se_theta_CL")
our_covadj_theta_est_se
```

We can compare that with the above calculated unadjusted standard error:

```{r}
our_theta_est_se <- attr(unadj_score_solution$f.root, "se_theta_L")
our_theta_est_se
```

So we can see that the covariate adjusted standard error is indeed smaller than the unadjusted one.

### Comparison with `survival` results

Let's compare this with the `survival` package results using a Cox model:

```{r}
check_adjusted <- coxph(Surv(time, status_numeric) ~ sex + age + ph.karno + meal.cal, data = dat, ties = "breslow")

coxph_covadj_theta_est <- as.numeric(coef(check_adjusted)["sexMale"])
coxph_covadj_theta_est

coxph_covadj_theta_est_se <- sqrt(vcov(check_adjusted)["sexMale", "sexMale"])
coxph_covadj_theta_est_se
```

So these results are different, which is expected, because with this Cox model we are estimating a conditional hazard ratio.

### Comparison with RobinCar results

We can also compare this with the results from RobinCar:

```{r}
library(RobinCar)
cl <- robincar_covhr(
    df = df_subset,
    treat_col = "sex",
    response_col = "time",
    event_col = "status_numeric",
    car_strata_cols = NULL,
    covariate_cols = c("age", "ph.karno", "meal.cal"),
    car_scheme = "simple",
    adj_method = "CL",
    ref_arm = "Female",
    p_trt = mean(df_subset$sex == "Male")
)
str(cl)

robincar_theta_est <- cl$result$theta_CL
robincar_theta_est

robincar_theta_est_se <- cl$result$se_theta_CL
robincar_theta_est_se
```

So these results are close but not numerically equal to what we obtained earlier:

```{r}
our_covadj_theta_est - robincar_theta_est
our_covadj_theta_est_se - robincar_theta_est_se
```

We will need to dig a little bit to understand why the results are not numerically equal. It could be e.g. due to the handling of ties.

# References
